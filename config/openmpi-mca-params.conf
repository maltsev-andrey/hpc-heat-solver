# Permanent OpenMPI Configuration for HPC+GPU Cluster
# This file should be placed in ~/.openmpi/mca-params.conf on all nodes

# Force TCP transport (stable for dual-network setup)
btl = tcp,self,vader

# Use internal network for compute nodes
btl_tcp_if_include = 10.10.10.0/24,170.168.1.0/24

# Explicitly exclude problematic interfaces
btl_tcp_if_exclude = lo,docker0,virbr0

# Define private networks
opal_net_private_ipv4 = 10.10.10.0/24,192.168.0.0/16,172.16.0.0/12

# Disable OFI/libfabric (causes subnet issues)
mtl = ^ofi

# Performance tuning
btl_tcp_sndbuf = 0
btl_tcp_rcvbuf = 0
btl_tcp_rdma_pipeline_send_length = 1048576
btl_tcp_rdma_pipeline_frag_size = 1048576

# Process binding for performance
hwloc_base_binding_policy = core
rmaps_base_mapping_policy = slot

# Increase timeout for large jobs
mpi_preconnect_all = 1
orte_abort_timeout = 60
